{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 6 – Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/jdecorte/machinelearning/blob/main/060-decision_trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.7 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 7)\n",
    "\n",
    "# Scikit-Learn ≥1.01 is required\n",
    "from packaging import version\n",
    "import sklearn\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Like SVMs, Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. \n",
    "- Powerful algorithms, capable of fitting complex datasets.\n",
    "- Fundamental components of Random Forests (see next chapter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build one and take a look at how it makes predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y_iris = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_iris, y_iris)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the trained Decision Tree by first using the `export_graphviz()` method to output a graph definition file called iris_tree.dot.  \n",
    "\n",
    "You first have to:\n",
    "- pip install graphviz (or py -m pip install graphviz)\n",
    "- install the dot executable from http://www.graphviz.org/download/ (indicate to add the executables to the OS PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "IMAGES_PATH = \".\"\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can execute  \n",
    "\n",
    "  `dot -Tpng iris_tree.dot -o iris_tree.png`\n",
    "\n",
    "from the command prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'iris_tree.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpimg\u001b[39;00m\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m9\u001b[39m,\u001b[39m9\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m img \u001b[39m=\u001b[39m mpimg\u001b[39m.\u001b[39;49mimread(\u001b[39m'\u001b[39;49m\u001b[39miris_tree.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m imgplot \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mimshow(img)\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\kmer422\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:1541\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(parse\u001b[39m.\u001b[39murlparse(fname)\u001b[39m.\u001b[39mscheme) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1535\u001b[0m     \u001b[39m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1537\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease open the URL for reading and pass the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1538\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresult to Pillow, e.g. with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1539\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1540\u001b[0m         )\n\u001b[1;32m-> 1541\u001b[0m \u001b[39mwith\u001b[39;00m img_open(fname) \u001b[39mas\u001b[39;00m image:\n\u001b[0;32m   1542\u001b[0m     \u001b[39mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[0;32m   1543\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mPngImagePlugin\u001b[39m.\u001b[39mPngImageFile) \u001b[39melse\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m             pil_to_array(image))\n",
      "File \u001b[1;32mc:\\Users\\kmer422\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\ImageFile.py:105\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[1;34m(self, fp, filename)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodermaxblock \u001b[39m=\u001b[39m MAXBLOCK\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m is_path(fp):\n\u001b[0;32m    104\u001b[0m     \u001b[39m# filename\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(fp, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m fp\n\u001b[0;32m    107\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'iris_tree.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "plt.figure(figsize=(9,9))\n",
    "img = mpimg.imread('iris_tree.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the tree represented above to make predictions if you find an iris an want to classify it. \n",
    "- start at the _root node_ (depth 0, at the top): \n",
    "- this node asks whether the flower’s petal length is smaller than or equal to 2.45 cm.\n",
    "    - if it is you move down to the root’s left child node (depth 1, left).\n",
    "    - this is a leaf node (it does not have any children), so it does not ask any more questions.\n",
    "    - the decision tree predicts that your flower is a _Iris setosa_\n",
    "- suppose you find another flower, and this time the petal length is greater than 2.45 cm\n",
    "    - You move down to the root’s right child node (depth 1, right), which is not a leaf node\n",
    "    - This node asks another question: is the petal width smaller than 1.75 cm?\n",
    "    - If it is, then your flower is most likely an _Iris versicolor_ (depth 2, left). \n",
    "    - If not, it is likely an _Iris virginica_ (depth 2, right).\n",
    "\n",
    "- `samples` = how many training instances node applies to\n",
    "- `value` = how many training instances of each class node applies to\n",
    "- `gini` = _impurity_ of node:\n",
    "\n",
    "\n",
    "$$\n",
    "G_i = 1 - \\sum_{k=1}^np_{i,k}^2\n",
    "$$\n",
    "where:\n",
    "- $p_{i,k}$ = ratio of class $k$ instances among the training instances in the $i^{th}$ node. \n",
    "\n",
    "---\n",
    "**NOTES** \n",
    "\n",
    "- One of the many qualities of Decision Trees is that they require very little data preparation. In fact, they\n",
    "don’t require feature scaling or centering at all.\n",
    "- Scikit-Learn uses the CART algorithm, which produces only binary trees: nonleaf nodes always have two children (i.e., questions only have yes/no answers). However, other algorithms such as ID3 can produce Decision Trees with nodes that have more than two children.\n",
    "---\n",
    "The figure below show this decision trees boundaries. \n",
    "![](img/decision_tree_decision_boundaries_plot.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the `max_depth` hyperparameter was set to 2 their are still impurities in the lefthand area. \n",
    "- If you set `max_depth` to 3, then the two depth-2 nodes would each add another decision boundary (represented by the dotted lines).\n",
    "\n",
    "---\n",
    "### MODEL INTERPRETATION: WHITE BOX VERSUS BLACK BOX\n",
    "\n",
    "- Decision Trees are intuitive, and their decisions are easy to interpret. \n",
    "- Such models are often called `white box` models. \n",
    "- In contrast, as we will see, Random Forests or neural networks are generally considered black `box models`.  \n",
    "    - They make great predictions, and you can easily check the calculations that they performed to make these predictions.\n",
    "    - Nevertheless, it is usually hard to explain in simple terms why the predictions were made. \n",
    "    - For example, if a neural network says that a particular person appears on a picture, it is hard to know what contributed to this prediction: did the model recognize that person’s eyes? Their mouth? Their nose? Their shoes? Or even the couch that they were sitting on? \n",
    "- Conversely, Decision Trees provide nice, simple classification rules that can even be applied manually if need be (e.g., for flower classification).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To estimate the probability that an instance belongs to a particular class $k$ it First it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class $k$ in this node.\n",
    "- Suppose you have found a flower whose petals are 5 cm long and 1.5 cm wide. \n",
    "    - The corresponding leaf node is the depth-2 left node, so the Decision Tree should output the following probabilities: \n",
    "        - 0% for Iris setosa (0/54)\n",
    "        - 90.7% for Iris versicolor (49/54)\n",
    "        - 9.3% for Iris virginica (5/54). \n",
    "    - If you ask it to predict the class, it should output Iris versicolor (class 1) because it has the highest probability.\n",
    "    - Notice that the estimated probabilities would be identical anywhere else in the bottom-right rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART training algorithm\n",
    "- Scikit-Learn uses the Classification and Regression Tree (CART) algorithm to train Decision Trees (also called “growing” trees). \n",
    "- The algorithm works by first splitting the training set into two subsets using a single feature $k$ and a threshold $t$ (e.g., “petal length ≤ 2.45 cm”).\n",
    "- It chooses $k$ and $t$ by searching for the pair $(k, t )$ that produces the purest subsets (weighted by their size). \n",
    "- The equation gives the cost function that the algorithm tries to minimize.\n",
    "$$\n",
    "J(k,t_k) = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right}\n",
    "$$\n",
    "where\n",
    "- $G_{left/right}$ measures the impurity of the left/right subset.\n",
    "- $m_{left/right}$ is the number of instances in the left/right subset.  \n",
    "\n",
    "Once the CART algorithm has successfully split the training set in two\n",
    "- it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. \n",
    "- it stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "- Predictions are very fast, even when dealing with large training sets. \n",
    "- Since trees are approximately balanced the overall prediction complexity is $O(log_2(m))$.\n",
    "- The training algorithm is $O(n \\times m log_2(m))$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters\n",
    "- If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely—indeed -> overfitting.\n",
    "- To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom during training (= regularization).\n",
    "    - `max_depth`: maximum depth of tree  (default = None = unlimited)\n",
    "    - `min_samples_split`: minimum number of samples a node must have before it can be split\n",
    "    - `min_samples_leaf`: minimum number of samples a leaf node must have\n",
    "    - `min_weight_fraction_leaf`: same as `min_samples_leaf` but expressed as a fraction of the total number of weighted instances\n",
    "    - `max_leaf_nodes`: maximum number of leaf nodes\n",
    "    - `max_features`: maximum number of features that are evaluated for splitting at each node\n",
    "- Increasing `min_*` hyperparameters or reducing `max_*` hyperparameters will regularize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows two Decision Trees trained on the moons dataset\n",
    "- On the left the Decision Tree is trained with the default hyperparameters (i.e., no restrictions) &rarr;  overfitting\n",
    "- On the right it’s trained with `min_samples_leaf=4`.\n",
    "![](img/min_samples_leaf_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are also capable of performing regression tasks. Let’s build a regression tree using Scikit-Learn’s `DecisionTreeRegressor` class, training it on a noisy quadratic dataset with max_depth=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic training set + noise\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "        tree_reg,\n",
    "        out_file=os.path.join(IMAGES_PATH, \"regression_tree.dot\"),\n",
    "        feature_names=[\"x1\"],\n",
    "        rounded=True,\n",
    "        filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](regression_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It predicts a values iso a class in each node. \n",
    "- For example, suppose you want to make a prediction for a new instance with x = 0.6. \n",
    "- You traverse the tree starting at the root, and you eventually reach the leaf node that predicts value=0.111. \n",
    "- This prediction is the average target value of the 110 training instances associated with this leaf node, and it\n",
    "results in a mean squared error equal to 0.015 over these 110 instances.\n",
    "\n",
    "- This model’s predictions are represented on the left in the figure below. \n",
    "- If you set max_depth=3, you get the predictions represented on the right. \n",
    "- Notice how the predicted value for each region is always the average target value of the instances in that region. \n",
    "- The algorithm splits each region in a way that makes most training instances as close as possible to that predicted value.\n",
    "\n",
    "![](img/tree_regression_plot.png)\n",
    "\n",
    "The CART algorithm works mostly the same way as earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that **minimizes the MSE**.\n",
    "\n",
    "Just like for classification tasks, Decision Trees are prone to overfitting when dealing with regression tasks:\n",
    "\n",
    "- Without any regularization (i.e., using the default hyperparameters), you get the predictions on the left. These predictions are obviously overfitting the training set very badly. \n",
    "- Just setting `min_samples_leaf=10` results in a much more reasonable model, represented on the right:\n",
    "![](img/tree_regression_regularization_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with one million instances?\n",
    "1. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth?\n",
    "1. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
    "1. Check the gini impurity in the orange and green node in the first decision tree above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "nav_menu": {
   "height": "309px",
   "width": "468px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "6e919314690ba078a9c4cab482e45708028526432cf77d9e4e02ebe923b94e34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
